# Deep Learning 응용
## 특강자 : 권택순 ESTsoft CTO


## 1교시

### 영상시청 (https://youtu.be/Wdpxrgd5XfI AI가 기사 쓰고 팩트체크까지... 기자는 사라질까?_210509  출처 : Youtube 질문하는 기자들 Q) 

```
- 구성도
    [input]                                                    [output]
    
    텍스트(대본) -> 목소리생성AI  
                       +        -> 얼굴생성 AI -> 병합 ->  주어진 텍스트를 
                   얼굴인식 AI                                읽는 AI 
                                                          아나운서 동영상
```    

- 과거에는 불가능했던 AI 표정기술이 지금은 gpt와 같은 자연어분석이 가능하면서 텍스트를 분석하여 거기에 맞는 표정기술을 구현해 낼 수 있음
- 감정 노동에 취약한 직업군, 시니어 캐어 등 인간 대신 AI가 대신 투입되어 이용됨
- Virtual Human을 통해 전도연, 송강호 같은 감정 연기 전문 배우 생성이 최종 목표


### 1. 지난 시간 remind
- IT : Infomation Technology
    - 정보를 어떻게 표현하는가? 정보를 어떻게 저장하는가?
    - 인코딩된 raw data를 가공하여 용량을 줄이고 더 선명한 모습으로 디코딩 (들어간 데이터와 나온 데이터의 모습이 조금 달라져도 목적에 맞게 잘 가공되어지는 것)
    - <strong>먼거리로 통신할 때 최대한 정확하고 노이즈가 거의 포함되지 않는 클린한 정보를 전달할 수 있도록 정보를 가공하여 전달하기 위한 기술</strong>
    - Denoisiong Autoencoder : 노이즈를 제거하는 자동인코더
    - image의 정교화 (ex. 360도 회전정교화)



## 2교시
### 1. 목소리 생성 AI
1. 인코더
- 텍스트(대본)를 input 하고 데이터를 잘게 쪼개서 generate 한다.
- word 임베딩(2012년) : 문장을 기반으로 단어 하나하나를 유추하여 임베딩 하는 기술 (성별, 동사 시제, 나라별 수도 등)
- text to speech(2006년 ~) : 텍스트를 WAVE로 output 함 (?)

2. 디코더
- 음성 데이터를 out 한다.
- 오디오 딥러닝을 딥하게 하겠다? -> WAVE(1차원 데이터)를 그대로 다루지 않고 그림(2차원 데이터, 시각화된 강도 데이터)으로 다루겠다.(멜스펙토그램)
    - Mel Spectrogram이란, 소리의 파형을 인간이 들을 수 있는 범위로 줄인 Mel scale로 다운 스케일한 이후 그 파형을 그림으로 그린 모양
- 인공지능이 그림을 훨씬 더 잘 다루기 때문


### 2. ChatGPT
1. ChatGPT 사용하는 경우?
- 코딩하기 전 초안 같은 느낌으로 사용
- 강의 전 개요 짜기
- 영어공부 시....
- 데이터 전처리 시

2. ChatGPT 사용하기 위해 갖춰야 할 부분
- 내가 얻고자 하는 정보에 대해 ChatGPT에게 어떤식으로 질문해야 하는지
- 주어진 정보를 보고 수정해야 하는 부분을 파악할 수 있는 안목 (정보교정)


## 3교시

### 1. 실습 (https://colab.research.google.com/drive/1NSN6gNLaudf4dfuz3NDRHoitrUoU1VCn)

1. ffmepg
- c, c++ 로 만들어진 라이브러리

2. openAI가 개발한 것들
- ChatGPT
- DALL.E
- Whisper
    - Whisper 설치
    - ! pip install git+https://github.com/openai/whisper.git -q

3. GPU를 인공지능이 쓰고 있는 이유
- 병렬연산이 처리에 유리해서 (알고리즘을 하는데 하나의 알고리즘을 여러 데이터에 적용하여 처리)

4. type='cuda'
- CUDA는 그래픽 처리 장치에서 수행하는 알고리즘을 C 프로그래밍 언어를 비롯한 산업 표준 언어를 사용하여 작성할 수 있도록 하는 GPGPU 기술

    ```
    아래에 device(type='cuda', index=0) 출력이 표시되어야 합니다.
    그렇지 않다면 CPU 전용 Colab 인스턴스를 사용 중일 수 있으며 느리게 실행됩니다.
    런타임->런타임 유형 변경`으로 이 문제를 해결하세요.
    ```

5. !nvidia-smi

6. STT 수행 (https://github.com/openai/whisper)

7. 구글 gTTS
- !pip install gTTS










-----------------------------------------------------------------

# Deep Learning Language Models
## 특강자 : 변형진 ESTsoft Tech Center 센터장
- LLM 기본 원리 이해하기
- GPT 사용해보기
- Q&A

## 1교시
### 시연영상 (GPT 연동한 키오스크 데모버전)

### Deep Learning Language Models
1. GPT를 이루고 있는 기술들
- Transformer Architecture : 문장이나 문서에서 중요한 부분에 집중하고, 그것이 다른 부분과 어떻게 상호작용하는지 집중
- Pretraining and Find-tuning
- Generative Model : 입력을 가지고 새로운, 창조적인 출력 생성
- Token-based Processing : 토큰이라는 작은 단위로 나누어 처리

2. Natural Language Processing (NLP)
- Natural Language Understanding (NLU)
    - Text(단어) -> Meaning (의미. 머리속에서 상상한 무언가)

- Natural Language Generation (NLG)
    - Meaning(추상적 의미) -> Text(단어)

3. Supervised Learning
- 내가 단어를 줄테니까 너는 그걸 학습해.
- Sentiment Classification (감정 분류)
    - Sentiment : Positive, Negative
    - Emotion : laughing, Well, Sad, Shocked, Angry, Happy

- P(Y|X) - 감정을 학습시키기 (답을 도출시킴)

    ```
        - I love this movie. I've seen it many times and it's still awesome. => positive
        - This movie is bad. I don't like it it all. It's terrible. => negative
    ```

- 효율적이지만 dataset이 많이 필요하다. (문법적 이해, 뉘앙스의 이해)
    - This movie is not boring. -> Thumbs up (부정이지만 뜻은 긍정)
    - No movie si more boring than this one. -> Thumbs down (부정)
    - I can't say no to those who say no movie is more boring than this one -> Thumbs down (부정, 부정 => 복잡한 구조)

4. Transfer Learing
- 더 많은 데이터를 학습한 모델을 데려와서 더 specific한 문제를 학습하도록 교육
- ex`)` 온갖 다른 언어를 활용한 모델을 데리고 와서 한국어 학습을 시킴
- ex`)` 수십만장의 이미지를 학습한 모델을 데리고 와서 폐암에 대해 학습 시킴

5. Language Models
- 텍스트 데이터를 처리하고 이해하는 인공지능의 한 가지 방식
- 언어모델은 단어나 문장이 얼마나 자연스러운지, 또는 주어진 문맥에서 다음 단어나 문장이 무엇일지 예층하는데 사용 (기계번역, 맞춤법검사, 음성인식, 챗봇, 검색 엔진 등)
- 특정 단어나 문장의 확률을 계산하거나 주어진 문맥에서 다음 단어의 확률 예측
- 결합 확률

    - P(I, am, not, a, student) = P(I) * P(am | I) * P(not | I, am)
                                    * P(a | I, am, not) 
                                    * P(student | I, am, not, a)
                                    => 확률이 대부분 높음

    - P(He, am, students) = P(He) * P(am | He) * P(student | He, am)
                            => 확률이 대부분 낮음

- 글의 앞부분을 모델에게 주었을 때 모델이 글의 뒷부분을 완성시킬 수 있음.
- 질문을 던졌을 때 해당하는 답변을 도출시킬 수 있음

6. Recurrent Neural Networks(RNN)
- FNN : input을 한번에 넣고 계산
- RNN : Recurrent Neural Networks (input을 하나씩 넣어 그 다음 것을 계산)

7. RNNs for Sequence Tagging
- Part-of-speech tagging, named entity recognition
- Sentiment classification (Positive인지, negative인지...)
    - Basic way : Use final hidden state
    - usuall better : 

8. RNN-LMs for Text Generation
- Speech recognition, machine translation, summarization
    ex. Input(audio) ---conditioning---> RNN-LM(start : what's the weather)

9. Machine Translation
- RNN으로 하기 힘든 작업일 수 있음 : 어순이 다른 언어들의 번역 어려움

10. Sequence-To-Sequence Model
- bottle neck 현상 : 첫 단어는 큰 영향을 주지 않지만 마지막 단어에만 큰 영향을 받게 되는 현상
- 인코딩을 해서 어떤 백터 하나를 만든 다음 그 다음을 예측하게 하여 확률이 높은 다음 단어를 도출
- Encoding of the source sentence. Provieds initial hidden state for Decoder RNN. (원본 문장의 인코딩입니다. 디코더 RNN의 초기 숨겨진 상태를 제공합니다.)
- Decoder RNN is a Language Model that generats target sentence, conditioned on encoding. (디코더 RNN은 인코딩에 따라 대상 문장을 생성하는 언어 모델입니다.)


## 2교시
### Deep Learning Language Models
1. Sequence-to-sequence with Attention
- 가중치 평균을 더해 하나의 벡터를 만들어 그 다음에 올 단어를 예측하여 도출

2. Attention = Soft Lookup Table
- <strong>병렬처리가 가능</strong>
- GPU가 무한정 많으면 계산이 한번에 가능함 (수백단위의 단어가 존재해도 가능)
- In a lookup table, we have a table of keys that map to values. The query matches one of the keys, returning its value.
- In attions, the query matches al keys softly, to a weight between 0 and 1. The keys' values are multiplied by the weights and summed.
- Deep Learning 에서 query, keys는 벡터임.
- qeury와 가장 가까운 key 가 k4 라고 한다면 k4의 정보를 가장 많이 가져오게 됨. (가중치에 따라 정보를 가져오는 양도 다름)

3. Deep Learning 은 왜 1가지만 선택하는 것이 아닌 다중 선택 후 가중치의 합을 구하여 답을 도출하는 것인가?
- 오류를 조금씩 수정한다는 딥러닝의 개념과 달라지게 됨
- 오류가 될 수 있는 경로로 향하는 구간을 줄이고 답에 가까운 경로로 향하는 구간을 넓힘으로써 정답 도출 확률이 높아지게 됨
- 1가지 구간만 선택하게 되면 오류를 줄이는 것이 아닌 확실한 정답을 선택하는 방법이 됨

4. Self-Attention
- 만약 query가 같다면?
- 문장 스스로를 query 또는 key로 삼아 계산하면 관련이 깊은 것들 끼리 연결되어 구조를 만들것이다.
- 'it'이라는 단어가 가리키는 것이 무엇인지 선택하려면 sentiment한 단어도 고려하여 선택할 수 있도록 학습시킴

5. Multi-Head Attention
- <strong>Head를 여러개 만들어서 학습시킴</strong>
- 각각의 Head가 집중해야 되는 부분을 다르게 하여 학습
- Attention head 1 attends to entities
- Attention head 2 attends to syntactically relevant words

6. Transformers
- Multi-Head Self Attention으로 구성됨
- input, output 길이가 길어도 한번에 학습할 수 있다.
- 학습과정에 GPU만 많으면 한번에 input하고 한번에 output 할 수 있음
- 수천개의 GPU가 동시 계산할 수 있음 (1개의 GPU가 계산한 것을 다음 GPU가 계산하는 것이 아닌 여러개의 GPU가 동시에 계산되는 것)
- 언어에는 순서가 중요하지만, 순서가 사라짐

7. 단어 임베딩 (Word Representations)
- One-hot vectors
    - 'good' - 'bad' (거리 : 1, 각도 : 90˚)
    - 단어의 거리와 각도가 달라야 패턴화를 시킬 수 있다.
    - 차원을 줄여나가기 위해 비슷한 단어는 가깝게, 비슷하지 않은 단어는 멀어지게
    - 단어의 사이즈만큼의 벡터를 가짐

- Word Embeddings
    - turning into banking crises as ....
    - banking에 대한 의미적인, 문법적인 지위를 알기위해 주변 단어와의 관계를 계산
    


## 3교시
### Deep Learning Language Models
1. Subword Tokenizer
- Tokenizer
    - 생성, 비용에 영향을 줌

- openAI Platform (Tokenizer)
    - 단어 사이즈를 10000으로 제안할 때, 자주 등장한 것들 순서대로 단어당 ID를 부여하고, 10000에 포함되지 않은 자주 등장하지 않은 단어들은 적절히 Token 단위로 쪼갠다.
    - 통계적으로 영어는 1.x개의 token으로 단어가 쪼개지고, 한국어는 2.x개의 token으로 쪼개진다... 

2. Scaling Laws
- Scaling Laws for Neural Language models
    - 오버피팅 : 모델의 사이즈가 커지면 학습데이터를 그대로 외워서 오히려 학습데이터에서 보지 못한 데이터는 부정확하게 대답하는 현상
    - 오캄의 면도날 (가장 간단한 과정만 가지고 설명할 수 있는것)
    - <strong>Scaling Laws</strong> : <em>Attension의 시대에서는 사이즈를 더 키워야 Model의 정확도도 더 좋아진다!</em>

3. GPT-3
- zero-shot : 예시를 보여주지 않고 답을 도출
- One-shot : 예시를 보여주고 따라하게 하는 것
- Few-shot : 규칙을 발견할 수 있도록 여러개의 예시를 보여줌
- Fine-tuning : 수 많은 예시들을 보여주고 그것들을 학습하도록 함

4. InstructGPT
- Instruct : 지시의 학습을 튜닝한 것
- step 1 : Collect demonstration data, and train a supervised policy.
    - A prompt is sample from our prompt dataset.
    - A labeler demonstrates the desired output behavior.
    - This data is used to find-tune GPT-3 with supevised learning.
- step 2 : Collect comparison data, and train a reward model
    - A prompt and several model outputs are sampled
    - A labeler ranks the outputs from best to worst.
    - This data is used to train our reward model.
- step 3 : Optimize a policy against the reward usin reinforcement learning.
    - A new prompt is sampled from the dataset.
    - The policy generates an output.
    - The reward model calculates a reward for the output.
    - The reward is used to update the policy using PPO.

5. ChatGPT (GPT3.5 + InstructGPT)
- 





## 4교시
### 실습 (https://platform.openai.com/playground?mode=chat)

```
import os
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

response = openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "너에 대해서 이야기 해줘"
    },
    {
      "role": "assistant",
      "content": "저는 OpenAI에서 개발된 인공지능 챗봇입니다."
    },
    {
      "role": "assistant",
      "content": "저는 OpenAI에서 개발된 인공지능 챗봇입니다."
    },
    {
      "role": "assistant",
      "content": "저는 OpenAI에서 개발된 인공지능 챗봇입니다. 주로 사용자의 질문에 답변하거나 대화를 도와주는 역할을 합니다. 제 기술은 큰 데이터셋을 학습하여 사용자와의 대화에서 가장 적절한 응답을 생성하도록 프로그래밍되었습니다. 저는 언어를 이해하고 생성하는 데 특화되어 있으며, 텍스트 기반으로 작동합니다. 나에 대해 더 자세히 알고 싶다면 더 많은 질문이나 명령을 주세요!"
    }
  ],
  temperature=1,
  max_tokens=256,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0
)
```
```
Q . 구글에서 웹 검색을 해서 나온 검색 결과에서 웹 페이지를 크롤링 할 수 있는 파이썬 코드를 짜줘

A.
다음은 파이썬에서 구글 검색 결과 페이지를 크롤링하는 코드 예시입니다.

```python
import requests
from bs4 import BeautifulSoup

query = "검색어"  # 크롤링할 검색어 입력
page = 1  # 크롤링할 페이지 수 입력

def get_search_results(query, page):
    url = f"https://www.google.com/search?q={query}&start={page*10}"
    headers = { "User-Agent" : "Mozilla/5.0 (Windows NT 10.0;Win64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36" }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    search_results = []
    for result in soup.select(".r a"):
        link = result.get("href")
        link = link.replace("/url?q=", "")
        search_results.append(link)

    return search_results

results = get_search_results(query, page)
for link in results:
    print(link)


이 코드는 `requests`와 `BeautifulSoup` 모듈을 사용합니다. `get_search_results()` 함수는 주어진 검색어와 페이지에 대한 구글 검색 결과 링크를 가져옵니다. `main` 함수에서는 검색 결과 링크를 출력합니다.

참고: 이 코드는 검색 결과 페이지에 직접 요청을 보내고 크롤링하는 방식이기 때문에 스크래핑의 대상인 사이트의 로봇 배제 표준에 위배될 수 있습니다. 스크래핑을 실행할 때는 해당 사이트의 이용 약관을 확인하고, 사이트의 로봇 배제 표준을 준수해야 합니다.
```

## 5교시
1. 할루시네이션(hallucination)
- 트레이닝 데이터를 통해 판단하지 않는 것처럼 비쳐지는 AI에 의한 확신적 답변이다.[1] 예를 들어, 테슬라의 소득과 관련한 트레이닝 데이터가 없는 챗봇의 경우 내부적으로 알고리즘이 높은 신뢰도로 순위를 올리는 무작위 숫자(예: "$13.6 billion")를 생성한 다음 사실이 아닌 방향으로 움직이면서 반복적으로 테슬라의 소득이 $13.6 billion으로 언급하며 해당 수치가 생성 알고리즘의 약점의 결과임을 드러내지는 않는다.[2]

이러한 현상을 이른바 "할루시네이션"이라고 하는데 이는 인간심리학에서의 환각 현상에 빗댄 것이다.

2. scalability : 확장성
 - 확장성, 사용자수의 증대하는데 따라 컴퓨터 시스템이 더 커지고 강력하게 대응할 수 있는 정도

